from langchain.tools import tool
from langchain_core.documents import Document
import time

from langgraph.prebuilt.tool_executor import ToolExecutor
from langchain.tools.render import format_tool_to_openai_function
from langchain.agents import Tool
from langchain_experimental.utilities import PythonREPL
from prefect import flow, task
from prefect.task_runners import ConcurrentTaskRunner, ThreadPoolTaskRunner
from prefect.logging import get_run_logger

from common_tools.helpers.llm_helper import Llm
from common_tools.helpers.ressource_helper import Ressource
from common_tools.models.logical_operator import LogicalOperator
from common_tools.models.question_analysis import QuestionAnalysis, QuestionAnalysisPydantic
from common_tools.rag.rag_filtering_metadata_helper import RagFilteringMetadataHelper
from common_tools.rag.rag_service import RagService
from common_tools.rag.rag_inference_pipeline.rag_inference_pipeline import RagInferencePipeline
from langchain_core.language_models import BaseChatModel
from langchain_core.documents import Document
from langchain_core.runnables import Runnable, RunnablePassthrough
from langchain_core.prompts import ChatPromptTemplate

class RagInferencePipelineWithPrefect:
    
    def __init__(self, rag: RagService, default_filters: dict = {}, tools: list = None):
        self.rag: RagService = rag
        self.default_filters = default_filters
        self.tools = tools

        if tools and any(tools):
            python_repl = PythonREPL()
            repl_tool = Tool(
                name="python_repl",
                description="A Python shell. Use this to execute python commands. Input should be a valid python command. If you want to see the output of a value, you should print it out with `print(...)`.",
                func=python_repl.run,
            )

            # Pass the tool to the agent supervisor & to the tool executor
            all_tools = [repl_tool]
            
            additionnal_tools = [format_tool_to_openai_function(t) for t in tools] #TODO: check compatibility out of OpenAI
            all_tools.extend(additionnal_tools)
            self.rag.inference_llm = self.rag.inference_llm.bind_functions(all_tools)
            self.tool_executor = ToolExecutor(all_tools)


    # Main flow
    @flow(
            name="rag inference pipeline flow", 
            task_runner=ThreadPoolTaskRunner(max_workers=10),
            retries=0,
            log_prints=True)
    def run(self, query: str, include_bm25_retrieval: bool = False, give_score = True):
        logger = get_run_logger()
        logger.info("Starting rag inference pipeline")

        guardrails_result = self.guardrails_query_analysis.submit(query)
        
        # Pre-treatment query: translate, search for meta-data
        analysed_query, metadata = self.rag_pre_treatment(query)
        
        # Data Retrieval subflow (runs in parallel for rag and BM25)
        retrieved_chunks = self.rag_hybrid_retrieval(analysed_query, metadata, include_bm25_retrieval, give_score)

        # Augmented Answer Generation subflow
        response = self.rag_augmented_answer_generation(retrieved_chunks, analysed_query)

        # Post-treatment subflow
        final_response = self.rag_post_treatment(response)
        
        # Wait for guardrails analysis to finish        
        guardrails_result = guardrails_result.result()
        if not guardrails_result:
            print("Query rejected by guardrails")
            return "I cannot answer your question, because its topic is explicitly forbidden.", retrieved_chunks #todo: translate when needed

        min_score = 0.5
        if give_score and min(chunk[1] for chunk in retrieved_chunks) > min_score:
            print("Query rejected by guardrails")
            return "I cannot answer your question, as I found no relevant information about your query.", retrieved_chunks
        
        return final_response, retrieved_chunks
    
    # Pre-treatment subflow
    @flow(name="rag pre-treatment", task_runner=ThreadPoolTaskRunner(max_workers=3))
    def rag_pre_treatment(self, query):
        questionAnalysis = self.analyse_query_language(query)
        extracted_metadata = self.extract_explicit_metadata(questionAnalysis.translated_question)
        
        questionAnalysis.translated_question = extracted_metadata[0].strip()
        metadata = extracted_metadata[1]

        return questionAnalysis, metadata
    
    @task(log_prints=True)
    def guardrails_query_analysis(self, query) -> bool:
        # Logic for guardrails analysis (returns True if OK, False if rejected)
        time.sleep(5)
        if "bad query" in query:  # Example check
            return False
        #print(">>> Query accepted by guardrails")
        return True
    
    @task
    def analyse_query_language(self, query):
        prefilter_prompt = Ressource.get_language_detection_prompt()
        prefilter_prompt = prefilter_prompt.replace("{question}", query)
        prompt_for_output_parser, output_parser = Llm.get_prompt_and_json_output_parser(prefilter_prompt, QuestionAnalysisPydantic, QuestionAnalysis)
        response = Llm.invoke_parallel_prompts_with_parser_batchs_fallbacks("rag prefiltering", [self.rag.inference_llm, self.rag.inference_llm], output_parser, 10, *[prompt_for_output_parser])
        questionAnalysis = response[0]
        questionAnalysis['question'] = query
        if questionAnalysis['detected_language'].__contains__("english"):
            questionAnalysis['translated_question'] = query
        return QuestionAnalysis(**questionAnalysis)
    
    @task
    def extract_explicit_metadata(self, query) -> tuple[str, dict]:
        filters = {}
        if RagFilteringMetadataHelper.has_manual_filters(query):
            filters, query = RagFilteringMetadataHelper.extract_manual_filters(query)
        else:
            filters = self.default_filters       
        return query, filters

    # Data Retrieval sub-flow with parallel rag and BM25 retrieval
    @flow(name="rag hybrid retrieval", task_runner=ThreadPoolTaskRunner(max_workers=3))
    def rag_hybrid_retrieval(self, analysed_query: QuestionAnalysis, metadata, include_bm25_retrieval: bool = False, give_score: bool = True, max_retrived_count: int = 10):
        rag_retrieved_chunks = self.semantic_vector_retrieval.submit(analysed_query, metadata, give_score, max_retrived_count)
        if include_bm25_retrieval:
            bm25_retrieved_chunks = self.bm25_retrieval.submit(analysed_query.translated_question, metadata, give_score, max_retrived_count)

        rag_retrieved_chunks = rag_retrieved_chunks.result()
        bm25_retrieved_chunks = bm25_retrieved_chunks.result() if include_bm25_retrieval else []
        retained_chunks = RagInferencePipelineWithPrefect.hybrid_chunks_selection(rag_retrieved_chunks, bm25_retrieved_chunks, give_score, max_retrived_count)
        return retained_chunks

    @task    
    def hybrid_chunks_selection(rag_retrieved_chunks: list[Document], bm25_retrieved_chunks: list[Document] = None, give_score: bool = False, max_retrived_count: int = None):
        if not bm25_retrieved_chunks or not any(bm25_retrieved_chunks):
            return rag_retrieved_chunks
        
        rag_retrieved_chunks.extend([(chunk, 0) for chunk in bm25_retrieved_chunks] if give_score else bm25_retrieved_chunks)
        
        if max_retrived_count:
            if give_score:
                rag_retrieved_chunks = sorted(rag_retrieved_chunks, key=lambda x: x[1], reverse=True)
            rag_retrieved_chunks = rag_retrieved_chunks[:max_retrived_count]

        return rag_retrieved_chunks
    
    @task
    def semantic_vector_retrieval(self, analysed_query: QuestionAnalysis, filters, give_score: bool = False, max_retrived_count: int = 10, min_score: float = None, min_retrived_count: int = None):
        retrieved_chunks = self.rag.semantic_vector_retrieval(analysed_query.translated_question, None, filters, give_score, max_retrived_count, min_score, min_retrived_count)
        return retrieved_chunks
    
    @task
    def bm25_retrieval(self, analysed_query: QuestionAnalysis, filters: dict, give_score: bool, k = 3):
        if filters and any(filters):
            filtered_docs = [doc for doc in self.rag.langchain_documents if RagInferencePipeline.filters_predicate(doc, filters)]
        else:
            filtered_docs = self.rag.langchain_documents

        bm25_retriever = self.rag._build_bm25_retriever(filtered_docs, k)#, filters
        bm25_retrieved_chunks = bm25_retriever.invoke(analysed_query.translated_question)
       
        if give_score:
            score = 0.1
            return [(doc, score) for doc in bm25_retrieved_chunks]
        else:
            return bm25_retrieved_chunks
        
     # Helper function to evaluate filter(s) (handling nested operators)
    @staticmethod
    def filters_predicate(doc, filters, operator=LogicalOperator.AND):
        # If filters is a dictionary (single filter or operator like $and/$or)
        if isinstance(filters, dict):
            if "$and" in filters:
                # Recursively handle $and with multiple conditions
                return RagInferencePipeline.filters_predicate(doc, filters["$and"], LogicalOperator.AND)
            elif "$or" in filters:
                # Recursively handle $or with multiple conditions
                return RagInferencePipeline.filters_predicate(doc, filters["$or"], LogicalOperator.OR)
            else:
                # Handle single key-value filter (field-value pair)
                return all(doc.metadata.get(key) == value for key, value in filters.items())

        # If filters is a list, apply the operator (AND/OR)
        elif isinstance(filters, list):
            if operator == LogicalOperator.AND:
                return all(
                    RagInferencePipeline.filters_predicate(doc, sub_filter, LogicalOperator.AND)
                    for sub_filter in filters
                )
            elif operator == LogicalOperator.OR:
                return any(
                    RagInferencePipeline.filters_predicate(doc, sub_filter, LogicalOperator.OR)
                    for sub_filter in filters
                )
            else:
                raise ValueError(f"Unhandled operator: {operator}")

        return False
    
    
    # Augmented answer generation subflow
    @flow(name="rag answer generation")
    def rag_augmented_answer_generation(self, retrieved_chunks: list, questionAnalysis: QuestionAnalysis):
        return self.rag_response_generation(retrieved_chunks, questionAnalysis)

    @task
    def rag_response_generation(self, retrieved_chunks: list, questionAnalysis: QuestionAnalysis):
        # Remove score from retrieved docs
        retrieved_chunks = [doc[0] if isinstance(doc, tuple) else doc for doc in retrieved_chunks]
        return self.generate_augmented_response_from_retrieved_chunks(self.rag.inference_llm, retrieved_chunks, questionAnalysis)
        
    # Post-treatment subflow
    @flow(name="rag post-treatment")
    def rag_post_treatment(self, response):
        return self.response_post_treatment(response)
        
    @task
    def response_post_treatment(self, guardrails_answer: bool, rag_answer: str, analysed_query: QuestionAnalysis):
        if guardrails_answer == True:
            return rag_answer
        else:
            if analysed_query.detected_language == "french":
                return "Je ne peux pas répondre à votre question, car son sujet est explicitement interdit."
            else:
                return "I cannot answer your question, because its topic is explicitly forbidden."
    
    @staticmethod
    def generate_augmented_response_from_retrieved_chunks(self, llm: BaseChatModel, retrieved_docs: list[Document], questionAnalysis: QuestionAnalysis, format_retrieved_docs_function = None) -> str:
        retrieval_prompt = Ressource.get_rag_augmented_generation_query_prompt()
        retrieval_prompt = retrieval_prompt.replace("{question}", questionAnalysis.translated_question)
        additional_instructions = ''
        if not questionAnalysis.detected_language.__contains__("english"):
            additional_instructions = Ressource.get_prefiltering_translation_instructions_prompt()
            additional_instructions = additional_instructions.replace("{target_language}", questionAnalysis.detected_language)
        retrieval_prompt = retrieval_prompt.replace("{additional_instructions}", additional_instructions)
        rag_custom_prompt = ChatPromptTemplate.from_template(retrieval_prompt)

        if format_retrieved_docs_function is None:
            context = retrieved_docs
        else:
            context = format_retrieved_docs_function(retrieved_docs)
        
        rag_chain = rag_custom_prompt | llm | RunnablePassthrough()
        answer = rag_chain.invoke(input= context)

        return Llm.get_content(answer)  